# MPI ALELEON Supercomputer

Halaman ini menjelaskan teknis menjalankan program **MPI** di ALELEON Supercomputer secara **[batch job]** melalui **[submit script]**. Ditujukan untuk **user yang hendak mendalami** dan menjalankan program MPI **dengan konfigurasi khusus**.

> Merupakan **halaman pendalaman info**:
> * *Tidak esensial untuk panduan umum.*
> 
> 

**- Daftar Isi -**

* [ALELEON Memilih Open MPI](https://www.google.com/search?q=%23aleleon-memilih-open-mpi)

1. [Bagaimana Slurm ALELEON Menjalankan MPI](https://www.google.com/search?q=%231-bagaimana-slurm-aleleon-menjalankan-mpi)
2. [Sintaks Menjalankan MPI di ALELEON](https://www.google.com/search?q=%232-sintaks-menjalankan-mpi-di-aleleon)
3. [Teknis dan Variasi SBATCH MPI CPU](https://www.google.com/search?q=%233-teknis-dan-variasi-sbatch-mpi-cpu)
1. [sausage slimit](https://www.google.com/search?q=%2331-sausage-slimit)
2. [Jumlah Node dan Memori per Node](https://www.google.com/search?q=%2332-jumlah-node-dan-memori-per-node)
3. [Opsi Skenario Menjalankan MPI CPU](https://www.google.com/search?q=%2333-opsi-skenario-menjalankan-mpi-cpu)


4. [Teknis dan Variasi SBATCH MPI GPU](https://www.google.com/search?q=%234-teknis-dan-variasi-sbatch-mpi-gpu)
1. [NVIDIA CUDA dan CUDA-aware MPI](https://www.google.com/search?q=%2341-nvidia-cuda-dan-cuda-aware-mpi)
2. [Opsi Skenario Proses MPI di CPU vs GPU](https://www.google.com/search?q=%2342-opsi-skenario-proses-mpi-di-cpu-vs-gpu)



---

## ALELEON Memilih Open MPI

ALELEON menggunakan **Open MPI** sebagai MPI utama yang dioptimasi mengikuti arsitektur **AMD Zen 2**. Daftar MPI terinstal di ALELEON dapat dilihat pada **[Katalog Software ALELEON - Compiler MPI]**.

---

## 1. Bagaimana Slurm ALELEON Menjalankan MPI

Slurm ALELEON Supercomputer menjalankan proses MPI untuk **selalu mengisi penuh core thread per core fisik**, contoh:

* 32 proses MPI dijalankan oleh 16 core fisik / 32 core thread CPU.
* 64 proses MPI dijalankan oleh 32 core fisik / 64 core thread CPU; dst...

Pembagian didasarkan pada pengelompokan core CPU pada grup CCX dan CCD AMD Zen2 supaya memastikan penyebaran proses MPI selalu berdekatan antar grup CCX / CCD *(tightly coupled)* dan menghasilkan performa terbaik.

---

## 2. Sintaks Menjalankan MPI di ALELEON

Sintaks untuk jumlah proses MPI mengacu **core CPU**:

```bash
# --- MPI berdasarkan jumlah alokasi CPU
mpirun -np ${SLURM_NTASKS} --use-hwthread-cpus \
    [sintaks-program-MPI-user...]

```

**Keterangan:**

1. Slurm ALELEON memberlakukan **1 proses MPI = 1 core thread CPU**.
2. Variabel `$SLURM_NTASKS` menangkap keseluruhan jumlah alokasi core CPU.
* Mencakup berbagai kombinasi SBATCH yang berhubungan dengan alokasi CPU contoh `ntasks` * `cpus-per-task`, `nodes` * `ntasks-per-node`, dll.


3. Flag `--use-hwthread-cpus` untuk menyebarkan proses MPI ke thread CPU karena *hyperthreading* aktif pada semua CPU di ALELEON.
4. Teknis dan variasi SBATCH lihat subbab **[ 3 ]**.

Sintaks untuk jumlah proses MPI mengacu **GPU**:

```bash
# --- MPI berdasarkan jumlah alokasi GPU
mpirun -np ${SLURM_GPUS} [sintaks-program-MPI-user...]

```

**Keterangan:**

1. Perhatikan menyesuaikan **kompatibilitas dan hanya untuk** software tertentu.
2. Teknis dan variasi SBATCH lihat subbab **[ 4 ]**.

---

## 3. Teknis dan Variasi SBATCH MPI CPU

Berikut teknis, karakter, dan opsi alokasi komputasi Slurm ALELEON untuk menjalankan program MPI CPU:

### 3.1. sausage slimit

ALELEON menyediakan tool **[sausage, menu slimit]** untuk:

1. Memandu user menentukan besar SBATCH alokasi CPU dan memori untuk job MPI berbagai skenario.
2. Bekerja dengan teknis dibawah ini.

User dapat menggunakan tool sausage slimit **tanpa perlu** memahami penjelasan di bawah ini.

* *Gunakan penjelasan di bawah ini untuk penelusuran lebih lanjut / mendalam.*

### 3.2. Jumlah Node dan Memori per Node

Slurm ALELEON secara otomatis menyebar proses MPI dengan jumlah node:

| Jumlah Proses MPI | Partisi epyc | Partisi epyc-jumbo |
| --- | --- | --- |
| **2 - 128** | 1 | 1 |
| **129 - 256** | 2 | X *(maks 128)* |

Perhatikan bahwa SBATCH `mem` untuk alokasi memori berlaku **per node**, bukan keseluruhan, sehingga:

```bash
#SBATCH --mem=[alokasi memori per node]

# Total alokasi memori = SBATCH mem * jumlah node; atau
# Nilai SBATCH mem = jumlah memori yang diinginkan / jumlah node

```

**Contoh:**

* User menjalankan 32 proses MPI di partisi epyc dan butuh total alokasi memori 64GB.
* 32 proses MPI menggunakan 1 node.
* User mengisi SBATCH mem sebesar = 64GB / 1 = **64GB**


* User menjalankan 192 proses MPI di partisi epyc dan butuh total alokasi memori 400GB.
* 192 proses MPI menggunakan 2 node.
* User mengisi SBATCH mem sebesar = 400GB / 2 = **200GB**



### 3.3. Opsi Skenario Menjalankan MPI CPU

Berikut daftar opsi dan skenario berdasarkan penyebaran jumlah proses MPI:

1. Pure MPI
2. Hybrid MPI / OpenMP
3. Pure MPI pada Core Fisik
4. Pure MPI Multi-Node secara Manual
5. Hybrid MPI / OpenMP Multi-Node secara Manual

#### 3.3.1. Pure MPI

Pure MPI adalah program MPI pada umumnya tanpa protokol threading lainnya. Gunakan SBATCH `ntasks` untuk jumlah core thread CPU dan proses MPI untuk menjalankan Pure MPI dengan catatan:

1. Variabel `$SLURM_NTASKS` menangkap besar SBATCH `ntasks`.

```bash
#!/bin/bash
# Isi bagian yang ditandai 4 garing (////)

# Alokasi jumlah core thread CPU dan jumlah proses MPI 
#SBATCH --ntasks=////

# SBATCH lain dan alur jalannya program ....
////

# Perintah menjalankan program MPI
mpirun -np ${SLURM_NTASKS} --use-hwthread-cpus ////

```

*SBATCH ntasks dapat menyebar proses MPI sesuai tabel node default (lihat bagian 3.2).*

#### 3.3.2. Hybrid MPI / OpenMP

Adalah program MPI yang berjalan dengan **threading OpenMP (OMP) per proses MPI.** Gunakan kombinasi SBATCH `ntasks` dan `cpus-per-task` dengan catatan:

1. Total alokasi CPU = `ntasks` * `cpus-per-task`
2. Besar thread OMP = variabel `$SLURM_CPUS_PER_TASK`
3. Variabel `$SLURM_NTASKS` menangkap total CPU di atas.

```bash
#!/bin/bash
# Isi bagian yang ditandai 4 garing (////)

# ntasks mewakili jumlah proses MPI
#SBATCH --ntasks=////

# cpus-per-task mewakili jumlah thread OMP per proses MPI 
# Total alokasi CPU = ntasks * cpus-per-task
#SBATCH --cpus-per-task=////

# SBATCH lain dan alur jalannya program ....
////

# --- Definisi thread OMP
# Lihat penjelasan di bawah, sesuaikan dengan jenis OMP 

# Perintah menjalankan program MPI
# $SLURM_NTASKS mewakili ntasks * cpus-per-task
mpirun -np ${SLURM_NTASKS} --use-hwthread-cpus ////

```

Berikut opsi definisi thread OMP menyesuaikan library threading yang digunakan program MPI:

* Template submit script ALELEON telah menyesuaikan panduan ini.
* Apabila user membawa program sendiri, sesuaikan dengan dokumentasi program.
* Bila tidak diketahui jenis library threading, gunakan opsi GNU / standar.

```bash
# Threading OMP GNU / Standar
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}

# Threading OMP AMD BLIS (BLAS-like Library Instantiation Software)
export BLIS_JC_NT=1
export BLIS_IC_NT=${SLURM_CPUS_PER_TASK}
export BLIS_JR_NT=1
export BLIS_IR_NT=1

# Threading OMP OpenBLAS
threads=${SLURM_CPUS_PER_TASK}
export OPENBLAS_NUM_THREADS=${threads}
export GOTO_NUM_THREADS=${threads}
export OMP_NUM_THREADS=${threads}

# Threading OMP Intel MKL (Math Kernel Library)
threads=${SLURM_CPUS_PER_TASK}
export OMP_NUM_THREADS=${threads}
export MKL_NUM_THREADS=${threads}

```

#### 3.3.3. Pure MPI pada Core Fisik

Secara standar Slurm ALELEON menyebar proses MPI pada **core thread** CPU. Apabila dibutuhkan, user dapat menggunakan keseluruhan **core fisik** dengan ketentuan:

1. SBATCH `ntasks` **2x** jumlah proses MPI, atau jumlah proses MPI **1/2x** dari SBATCH `ntasks`.
* *Karena CPU ALELEON berjalan 2 thread per core fisik.*


2. Sintaks MPI tidak menggunakan flag `--use-hwthread-cpus`.
3. Perhatikan Slurm tetap menghitung core hour berdasarkan jumlah SBATCH `ntasks`.

```bash
#!/bin/bash
# Isi bagian yang ditandai 4 garing (////)

# Alokasi core CPU
# 2x dari jumlah proses MPI yang diinginkan
#SBATCH --ntasks=////

# SBATCH lain dan alur jalannya program ....
////

# Menghitung setengah nilai SBATCH ntasks
MPI_TASKS=$((${SLURM_NTASKS} / 2))

# Perintah menjalankan program MPI di core fisik
mpirun -np ${MPI_TASKS} ////

```

#### 3.3.4. Pure MPI Multi-Node secara Manual

User dapat mendefinisikan multi-node secara manual untuk menjalankan program Pure MPI melalui kombinasi SBATCH `nodes` dan `ntasks-per-node` dengan catatan:

1. Total alokasi CPU = `nodes` * `ntasks-per-node`
2. Variabel `$SLURM_NTASKS` menangkap total CPU di atas.
3. Jumlah node maksimal:
* Partisi epyc = 2
* Partisi epyc-jumbo = 1



```bash
#!/bin/bash
# Isi bagian yang ditandai 4 garing (////)

# Alokasi jumlah node 
#SBATCH --nodes=////

# Alokasi jumlah ntasks per node
# Total alokasi CPU = nodes * ntasks-per-node
#SBATCH --ntasks-per-node=////

# SBATCH lain dan alur jalannya program ....
////

# Perintah menjalankan program MPI
# $SLURM_NTASKS mewakili nodes * ntasks-per-node
mpirun -np ${SLURM_NTASKS} --use-hwthread-cpus ////

```

#### 3.3.5. Hybrid MPI / OpenMP Multi-Node secara Manual

Selain Pure MPI, user juga dapat menjalankan program hybrid MPI/OMP secara multi-node manual dengan catatan:

* SBATCH `cpus-per-task` berlaku **per node**.
* Total alokasi CPU = `nodes` * `ntasks-per-node` * `cpus-per-task`
* Variabel `$SLURM_NTASKS` menangkap total CPU di atas.
* Jumlah node maksimal:
* Partisi epyc = 2
* Partisi epyc-jumbo = 1



```bash
#!/bin/bash
# Isi bagian yang ditandai 4 garing (////)

# Alokasi jumlah node
#SBATCH --nodes=////

# ntasks mewakili jumlah proses MPI
#SBATCH --ntasks-per-node=////

# cpus-per-task mewakili jumlah thread OMP per proses MPI 
# Total alokasi CPU = nodes * ntasks-per-node * cpus-per-task
#SBATCH --cpus-per-task=////

# SBATCH lain dan alur jalannya program ....
////

# Definisi thread OMP
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

# Perintah menjalankan program MPI
# $SLURM_NTASKS mewakili ntasks * cpus-per-task
mpirun -np ${SLURM_NTASKS} --use-hwthread-cpus ////

```

---

## 4. Teknis dan Variasi SBATCH MPI GPU

Berikut teknis, karakter, dan opsi alokasi komputasi Slurm ALELEON untuk menjalankan program MPI GPU:

### 4.1. NVIDIA CUDA dan CUDA-aware MPI

ALELEON Supercomputer menggunakan GPU NVIDIA sehingga untuk utilisasinya butuh library CUDA dan *(idealnya)* MPI yang diinstal dengan library CUDA *(CUDA-aware MPI)*.

| Jenis MPI | Versi MPI | Versi CUDA | Nama Modul |
| --- | --- | --- | --- |
| Open MPI | 5.0.8 | 12.9 | `openmpi/5.0.8-gcc-15.2.0-cuda-12.9.0-neq` |

### 4.2. Opsi Skenario Proses MPI di CPU vs GPU

Umumnya pada program MPI GPU, user tetap **menjalankan proses MPI ke CPU** dimana proses MPI CPU tersebut mengirim paket komputasi ke GPU dengan catatan:

1. SBATCH `partition` **ampere** untuk akses compute node GPU.
2. SBATCH `ntasks` untuk jumlah core thread CPU dan proses MPI.
3. SBATCH `gpus` untuk alokasi jumlah GPU.
4. Variabel `$SLURM_NTASKS` menangkap besar SBATCH `ntasks`.

```bash
#!/bin/bash
# Isi bagian yang ditandai 4 garing (////)

# Partisi GPU
#SBATCH --partition=ampere

# Alokasi jumlah core thread CPU dan jumlah proses MPI 
#SBATCH --ntasks=////

# Alokasi jumlah GPU
#SBATCH --gpus=////

# SBATCH lain dan alur jalannya program ....
////

# Perintah menjalankan program MPI
mpirun -np ${SLURM_NTASKS} --use-hwthread-cpus ////

```

Hal sama untuk program MPI GPU **Hybrid MPI/OMP** yaitu berjalan dengan **threading OpenMP (OMP) per proses MPI.** Gunakan kombinasi SBATCH `ntasks` dan `cpus-per-task` dengan catatan:

1. Total alokasi CPU = `ntasks` * `cpus-per-task`
2. Besar thread OMP = variabel `$SLURM_CPUS_PER_TASK`
3. Variabel `$SLURM_NTASKS` menangkap total CPU di atas.

```bash
#!/bin/bash
# Isi bagian yang ditandai 4 garing (////)

# Partisi GPU
#SBATCH --partition=ampere

# ntasks mewakili jumlah proses MPI
#SBATCH --ntasks=////

# cpus-per-task mewakili jumlah thread OMP per proses MPI 
# Total alokasi CPU = ntasks * cpus-per-task
#SBATCH --cpus-per-task=////

# Alokasi jumlah GPU
#SBATCH --gpus=////

# SBATCH lain dan alur jalannya program ....
////

# Definisi thread OMP
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

# Perintah menjalankan program MPI
# $SLURM_NTASKS mewakili ntasks * cpus-per-task
mpirun -np ${SLURM_NTASKS} --use-hwthread-cpus ////

```

---

Hanya ada **beberapa** program MPI GPU yang dapat **menyebarkan proses MPI langsung ke tingkatan GPU**. Perhatikan dokumentasi program tersebut. Software komputasi ALELEON yang mampu menjalankan proses MPI di GPU adalah:

1. **[GROMACS]** versi CUDA.

Gunakan SBATCH `gpus` melalui variabel `$SLURM_GPUS` untuk jumlah proses MPI, menjadi:

```bash
#!/bin/bash
# Isi bagian yang ditandai 4 garing (////)

# Partisi GPU
#SBATCH --partition=ampere

# Alokasi jumlah core thread CPU dan jumlah proses MPI 
#SBATCH --ntasks=////

# Alokasi jumlah GPU
#SBATCH --gpus=////

# SBATCH lain dan alur jalannya program ....
////

# Perintah menjalankan program MPI langsung ke GPU
mpirun -np ${SLURM_GPUS} --use-hwthread-cpus ////

```