# =============================================================================
# docker-compose.yml — RAG + vLLM untuk AMD GPU (ROCm)
# =============================================================================
#
# Cara pakai:
#   docker compose up              → Jalankan vLLM server + RAG app
#   docker compose up vllm-server  → Jalankan vLLM server saja
#   docker compose run rag-app     → Jalankan RAG app saja (setelah server siap)
#
# Catatan:
#   - Sesuaikan HSA_OVERRIDE_GFX_VERSION dengan GPU Anda
#   - Model akan di-download otomatis ke ~/.cache/huggingface
#   - vLLM server butuh waktu untuk load model (~2-5 menit)
# =============================================================================

services:
  # ---- vLLM Server (ROCm) ----
  # Menjalankan LLM inference di GPU AMD
  vllm-server:
    image: vllmproject/vllm-openai:latest-rocm
    container_name: vllm-rocm
    devices:
      - /dev/kfd:/dev/kfd       # Kernel Fusion Driver (wajib ROCm)
      - /dev/dri:/dev/dri       # Direct Rendering Infrastructure
    group_add:
      - video
      - render
    environment:
      # Sesuaikan dengan GPU Anda:
      #   gfx1030  → RX 6800/6900   (RDNA2)
      #   gfx1100  → RX 7900 XTX    (RDNA3)
      #   gfx1151  → RX 9070 XT     (RDNA4) → 12.0.1
      - HSA_OVERRIDE_GFX_VERSION=12.0.1
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface  # Cache model agar tidak download ulang
    ports:
      - "8000:8000"
    command: >
      --model Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8
      --quantization fp8
      --gpu-memory-utilization 0.90
      --max-model-len 4096
      --trust-remote-code
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 300s  # Model loading bisa lama

  # ---- RAG App ----
  # Menjalankan script RAG (LangChain + Chroma + Embedding)
  rag-app:
    build:
      context: .
      dockerfile: Dockerfile.rocm
    container_name: rag-kristo
    environment:
      - VLLM_BASE_URL=http://vllm-server:8000/v1
      - MODEL_NAME=Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8
    depends_on:
      vllm-server:
        condition: service_healthy
    stdin_open: true   # Untuk mode interaktif nanti
    tty: true
